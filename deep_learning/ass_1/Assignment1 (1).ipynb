{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing multiple linear regression using numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1030 8\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Name : Student_1\n",
    "Roll No: roll_no\n",
    "\n",
    "Assignment 1a\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "'''\n",
    "You will not import any other library other than these provided.\n",
    "\n",
    "We provide the concrete_dataset as an example.\n",
    "There are 8 dependent variables columns(1-8).\n",
    "The last column (concrete compressive strength) is the value we wish to estimate.\n",
    "'''\n",
    "\n",
    "df= pd.read_csv('Concrete_Data.csv')\n",
    "df.head()\n",
    "\n",
    "# reads the file and stores in 2 numpy arrays.\n",
    "# X has the input features and Y has the output value in numpy array\n",
    "\n",
    "X = df.iloc[:,:-1].values\n",
    "Y = df.iloc[:,-1].values\n",
    "\n",
    "rows,cols= X.shape[0],X.shape[1]\n",
    "# how to get the number of rows and columns in the dataset.\n",
    "# Rows correspond to the number of input instances, columns correspond to the feature of an input\n",
    "\n",
    "print(rows,cols)\n",
    "\n",
    "np.random.seed(42) # to ensure that the same seed is generated\n",
    "\n",
    "# write code to shuffle the dataset\n",
    "\n",
    "def shuffle_dataset(X,Y):\n",
    "    \n",
    "    '''\n",
    "        Write code to shuffle the dataset here. \n",
    "        \n",
    "        Args: \n",
    "            X: Input feature ndarray\n",
    "            Y: Input values ndarray\n",
    "            \n",
    "        Return:\n",
    "            X and Y shuffled in place\n",
    "    \n",
    "    '''\n",
    "    np.random.shuffle(X,Y)\n",
    "    \n",
    "    pass\n",
    "\n",
    "training_size = int(0.8*rows)\n",
    "X_train = X[:training_size]\n",
    "y_train = Y[:training_size]\n",
    "X_test = X[training_size:]\n",
    "y_test = Y[training_size:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Linear Regression class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(object):\n",
    "    def __init__(self):\n",
    "        #Initialize all parameters\n",
    "        \n",
    "        self.w = np.random.randn(rows,cols)#? Sample an array corresponding to the number of input features (cols) from a uniform distribution between -1 and 1\n",
    "        self.b = np.random.randn(rows,cols) #? Sample from a uniform distribution between -1 and 1\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "            Do a forward pass of the classifier:\n",
    "            Args:\n",
    "                x: Input X matrix\n",
    "            Return:\n",
    "                y: y = X.w + b  \n",
    "        '''\n",
    "        # Complete this function \n",
    "        y=np.multiply(X,self.w)\n",
    "        y=y+self.b\n",
    "        return y\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    \n",
    "    def backward(self, x, ypred, y_train, lr):\n",
    "        '''\n",
    "            Computes all gradients and updates the parameters w and b\n",
    "            Args:\n",
    "                x : x\n",
    "                ypred: y=wx+b\n",
    "                y_train = ground truth values\n",
    "                lr = learning rate\n",
    "        '''\n",
    "        # Complete this function\n",
    "        \n",
    "        \n",
    "        raise NotImplementedError\n",
    "\n",
    "def MSELoss(y, ypred):\n",
    "    '''\n",
    "        Args:\n",
    "            y: ground truth labels\n",
    "            ypred: predicted labels\n",
    "        Return:\n",
    "            Mean squared error loss\n",
    "    '''\n",
    "    print (y.shape)\n",
    "    print (ypred.shape)\n",
    "    MSE=(y-ypred)*(y-ypred)\n",
    "    MSE.shape\n",
    "    MSE=np.average(MSE,0)\n",
    "    return MSE\n",
    "    # Compute the mean squared error \n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training with Gradient Descent\n",
      "(824,)\n",
      "(1030, 8)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (824,) (1030,8) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-1ffbf2b6fadd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mypred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlreg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# computes the predicted values\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMSELoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mypred\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# computes the MSE loss between the actual and predicted values\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[1;31m# store the values of loss per epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-63576e0bd653>\u001b[0m in \u001b[0;36mMSELoss\u001b[1;34m(y, ypred)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mypred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[0mMSE\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mypred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mypred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m     \u001b[0mMSE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[0mMSE\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMSE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (824,) (1030,8) "
     ]
    }
   ],
   "source": [
    "# Specify the number of epochs as well as the learning rate. \n",
    "# Keep the values fixed.\n",
    "\n",
    "print('Starting Training with Gradient Descent')\n",
    "lreg = LinearRegression()\n",
    "epochs = 10000\n",
    "learning_rate = 0.0000001\n",
    "\n",
    "loss_history = []\n",
    "epoch_history = []\n",
    "\n",
    "# Gradient Descent\n",
    "for e in range(epochs):\n",
    "    ypred = lreg.forward(X_train) # computes the predicted values\n",
    "    loss = MSELoss(y_train, ypred) # computes the MSE loss between the actual and predicted values\n",
    "    # store the values of loss per epoch\n",
    "    if e==0 or (e+1)%100==0:\n",
    "        loss_history.append(loss)\n",
    "        epoch_history.append(e+1)\n",
    "        \n",
    "    \n",
    "    lreg.backward(X_train, ypred, y_train, learning_rate)\n",
    "\n",
    "print('Loss fuction decrease after ' + str(epochs) + ' epochs of training')\n",
    "#Plot the decrease in loss with epoch\n",
    "plt.plot(epoch_history, loss_history)\n",
    "plt.show()\n",
    "\n",
    "print('Final training loss')   \n",
    "y_train_loss= 1# Print training loss ?\n",
    "print('Starting to test')\n",
    "ytest_pred=1 # find predictions on test set ?\n",
    "loss=1 # compute loss on test set ?\n",
    "print('Final test loss: ' + str(loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
